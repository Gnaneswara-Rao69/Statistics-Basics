{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Statistics Basics\n"
      ],
      "metadata": {
        "id": "kAGy-27rjbK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. How do you calculate the mean, median, and mode of a dataset?"
      ],
      "metadata": {
        "id": "AhjCIINrjw3K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUKTAFBtjSFg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Dataset\n",
        "data = [2, 4, 4, 6, 8]\n",
        "\n",
        "# Calculate mean\n",
        "mean = np.mean(data)\n",
        "\n",
        "# Calculate median\n",
        "median = np.median(data)\n",
        "\n",
        "# Calculate mode\n",
        "mode = stats.mode(data)\n",
        "\n",
        "print(\"Mean:\", mean)\n",
        "print(\"Median:\", median)\n",
        "print(\"Mode:\", mode)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a Python program to compute the variance and standard deviation of a dataset."
      ],
      "metadata": {
        "id": "HRlUeHf6kQGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset\n",
        "data = [2, 4, 4, 6, 8]\n",
        "\n",
        "# Calculate variance\n",
        "variance = np.var(data)\n",
        "\n",
        "# Calculate standard deviation\n",
        "std_dev = np.std(data)\n",
        "\n",
        "print(\"Variance:\", variance)\n",
        "print(\"Standard Deviation:\", std_dev)\n"
      ],
      "metadata": {
        "id": "WA_8h3dOknPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Create a dataset and classify it into nominal, ordinal, interval, and ratio types."
      ],
      "metadata": {
        "id": "XTyr-OgGkvIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a dataset\n",
        "data = {\n",
        "    \"Student ID\": [1, 2, 3, 4, 5],\n",
        "    \"Name\": [\"John\", \"Jane\", \"Bob\", \"Alice\", \"Mike\"],\n",
        "    \"Grade Level\": [\"Freshman\", \"Sophomore\", \"Junior\", \"Senior\", \"Freshman\"],\n",
        "    \"Satisfaction Rating\": [\"Very Satisfied\", \"Satisfied\", \"Neutral\", \"Dissatisfied\", \"Very Dissatisfied\"],\n",
        "    \"Score\": [85, 90, 78, 92, 88],\n",
        "    \"Age\": [18, 19, 20, 21, 18]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df)\n",
        "\n",
        "# Classify data types\n",
        "print(\"\\nData Types:\")\n",
        "print(df.dtypes)\n",
        "\n"
      ],
      "metadata": {
        "id": "TvigtIWVk0m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Implement sampling techniques like random sampling and stratified sampling."
      ],
      "metadata": {
        "id": "AMi_RRqlld-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create a sample dataset\n",
        "data = {\n",
        "    \"Feature1\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    \"Feature2\": [11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
        "    \"Target\": [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "## Random Sampling\n",
        "random_sample = df.sample(n=5, random_state=42)\n",
        "print(\"Random Sample:\")\n",
        "print(random_sample)\n",
        "\n",
        "## Stratified Sampling\n",
        "# Define features and target\n",
        "X = df[[\"Feature1\", \"Feature2\"]]\n",
        "y = df[\"Target\"]\n",
        "\n",
        "# Perform stratified sampling\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
        "\n",
        "# Combine features and target for train and test sets\n",
        "train_set = pd.concat([X_train, y_train], axis=1)\n",
        "test_set = pd.concat([X_test, y_test], axis=1)\n",
        "\n",
        "print(\"\\nStratified Train Set:\")\n",
        "print(train_set)\n",
        "print(\"\\nStratified Test Set:\")\n",
        "print(test_set)\n"
      ],
      "metadata": {
        "id": "3EmD6fT_lrfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a Python function to calculate the range of a dataset."
      ],
      "metadata": {
        "id": "PlkzfqlsmAby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_range(data):\n",
        "    return max(data) - min(data)\n",
        "\n",
        "# Example usage:\n",
        "data = [2, 4, 4, 6, 8]\n",
        "range_value = calculate_range(data)\n",
        "print(\"Range:\", range_value)\n"
      ],
      "metadata": {
        "id": "M0E2WKHOmFPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Create a dataset and plot its histogram to visualize skewness."
      ],
      "metadata": {
        "id": "5LCWIjabmNtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a skewed dataset\n",
        "np.random.seed(0)\n",
        "data = np.random.exponential(scale=1, size=1000)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data, columns=[\"Value\"])\n",
        "\n",
        "# Plot histogram\n",
        "plt.hist(df[\"Value\"], bins=30, alpha=0.7, color=\"blue\", edgecolor=\"black\")\n",
        "plt.title(\"Histogram of Skewed Data\")\n",
        "plt.xlabel(\"Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "qbPtuRDWmUKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  Calculate skewness and kurtosis of a dataset using Python libraries."
      ],
      "metadata": {
        "id": "PWzYsgX9my9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Create a dataset\n",
        "data = np.random.normal(0, 1, 1000)\n",
        "\n",
        "# Calculate skewness\n",
        "skewness = stats.skew(data)\n",
        "\n",
        "# Calculate kurtosis\n",
        "kurtosis = stats.kurtosis(data)\n",
        "\n",
        "print(\"Skewness:\", skewness)\n",
        "print(\"Kurtosis:\", kurtosis)\n"
      ],
      "metadata": {
        "id": "I3PDCR_Qnok-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Generate a dataset and demonstrate positive and negative skewness."
      ],
      "metadata": {
        "id": "8mYPNBeon6FV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "# Generate positively skewed dataset\n",
        "positive_skew = np.random.exponential(scale=1, size=1000)\n",
        "\n",
        "# Generate negatively skewed dataset\n",
        "negative_skew = -np.random.exponential(scale=1, size=1000)\n",
        "\n",
        "# Plot histograms\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "ax[0].hist(positive_skew, bins=30, alpha=0.7, color=\"blue\", edgecolor=\"black\")\n",
        "ax[0].set_title(\"Positive Skewness\")\n",
        "ax[0].set_xlabel(\"Value\")\n",
        "ax[0].set_ylabel(\"Frequency\")\n",
        "\n",
        "ax[1].hist(negative_skew, bins=30, alpha=0.7, color=\"red\", edgecolor=\"black\")\n",
        "ax[1].set_title(\"Negative Skewness\")\n",
        "ax[1].set_xlabel(\"Value\")\n",
        "ax[1].set_ylabel(\"Frequency\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "57l6COpBn_vU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  Write a Python script to calculate covariance between two datasets."
      ],
      "metadata": {
        "id": "Kf_nzOwNoPba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create two datasets\n",
        "np.random.seed(0)\n",
        "dataset1 = np.random.normal(0, 1, 100)\n",
        "dataset2 = np.random.normal(0, 1, 100)\n",
        "\n",
        "# Calculate covariance\n",
        "covariance = np.cov(dataset1, dataset2)[0, 1]\n",
        "\n",
        "print(\"Covariance:\", covariance)\n"
      ],
      "metadata": {
        "id": "BaHFg2OToTXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python script to calculate the correlation coefficient between two datasets."
      ],
      "metadata": {
        "id": "0ZfuXbxMoh5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using Numpy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Create two datasets\n",
        "np.random.seed(0)\n",
        "dataset1 = np.random.normal(0, 1, 100)\n",
        "dataset2 = 0.8 * dataset1 + np.random.normal(0, 0.2, 100)\n",
        "\n",
        "# Calculate correlation coefficient\n",
        "correlation_coefficient = np.corrcoef(dataset1, dataset2)[0, 1]\n",
        "\n",
        "print(\"Correlation Coefficient:\", correlation_coefficient)\n",
        "\n",
        "\n",
        "#Using Pandas\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create two datasets\n",
        "np.random.seed(0)\n",
        "dataset1 = np.random.normal(0, 1, 100)\n",
        "dataset2 = 0.8 * dataset1 + np.random.normal(0, 0.2, 100)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({\"Dataset1\": dataset1, \"Dataset2\": dataset2})\n",
        "\n",
        "# Calculate correlation coefficient\n",
        "correlation_coefficient = df[\"Dataset1\"].corr(df[\"Dataset2\"])\n",
        "\n",
        "print(\"Correlation Coefficient:\", correlation_coefficient)\n"
      ],
      "metadata": {
        "id": "quRsd1-vopm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Create a scatter plot to visualize the relationship between two variables."
      ],
      "metadata": {
        "id": "5KFPxDYKpCab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Create two datasets\n",
        "np.random.seed(0)\n",
        "x = np.random.normal(0, 1, 100)\n",
        "y = 0.8 * x + np.random.normal(0, 0.2, 100)\n",
        "\n",
        "# Create scatter plot\n",
        "plt.scatter(x, y, alpha=0.7, color=\"blue\", edgecolor=\"black\")\n",
        "plt.title(\"Scatter Plot of X vs Y\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7DPqwI1OpJGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Implement and compare simple random sampling and systematic sampling."
      ],
      "metadata": {
        "id": "pESNrMICpqjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Create a population dataset\n",
        "np.random.seed(0)\n",
        "population = np.arange(1, 101)\n",
        "\n",
        "## Simple Random Sampling\n",
        "random_sample = np.random.choice(population, size=10, replace=False)\n",
        "print(\"Simple Random Sample:\")\n",
        "print(random_sample)\n",
        "\n",
        "## Systematic Sampling\n",
        "k = 10  # sampling interval\n",
        "start_index = np.random.randint(0, k)\n",
        "systematic_sample = population[start_index::k]\n",
        "print(\"\\nSystematic Sample:\")\n",
        "print(systematic_sample)\n"
      ],
      "metadata": {
        "id": "H97u-M_0pyEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Calculate the mean, median, and mode of grouped data."
      ],
      "metadata": {
        "id": "OGA19ocRqfMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create grouped data\n",
        "data = {\n",
        "    \"Class Interval\": [\"0-10\", \"10-20\", \"20-30\", \"30-40\", \"40-50\"],\n",
        "    \"Frequency\": [5, 10, 15, 8, 2]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate midpoints\n",
        "midpoints = [(0+10)/2, (10+20)/2, (20+30)/2, (30+40)/2, (40+50)/2]\n",
        "df[\"Midpoint\"] = midpoints\n",
        "\n",
        "# Calculate mean\n",
        "df[\"Product\"] = df[\"Midpoint\"] * df[\"Frequency\"]\n",
        "mean = df[\"Product\"].sum() / df[\"Frequency\"].sum()\n",
        "print(\"Mean:\", mean)\n",
        "\n",
        "# Calculate median\n",
        "cf = df[\"Frequency\"].cumsum()\n",
        "median_class = df.loc[(cf >= df[\"Frequency\"].sum()/2).idxmax()]\n",
        "median = median_class[\"Midpoint\"] - ((df[\"Frequency\"].sum()/2 - (cf - median_class[\"Frequency\"]).max()) / median_class[\"Frequency\"]) * 10\n",
        "print(\"Median:\", median)\n",
        "\n",
        "# Calculate mode\n",
        "mode_class = df.loc[df[\"Frequency\"].idxmax()]\n",
        "mode = mode_class[\"Midpoint\"]\n",
        "print(\"Mode:\", mode)\n",
        "\n"
      ],
      "metadata": {
        "id": "JWTgrOfBqk8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.  Simulate data using Python and calculate its central tendency and dispersion."
      ],
      "metadata": {
        "id": "xvFmrI5bq9yS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Simulate data\n",
        "np.random.seed(0)\n",
        "data = np.random.normal(loc=5, scale=2, size=100)\n",
        "\n",
        "# Calculate central tendency\n",
        "mean = np.mean(data)\n",
        "median = np.median(data)\n",
        "mode = pd.Series(data).mode().values[0]\n",
        "\n",
        "print(\"Central Tendency:\")\n",
        "print(f\"Mean: {mean}\")\n",
        "print(f\"Median: {median}\")\n",
        "print(f\"Mode: {mode}\")\n",
        "\n",
        "# Calculate dispersion\n",
        "range_value = np.ptp(data)\n",
        "variance = np.var(data)\n",
        "std_dev = np.std(data)\n",
        "iqr = np.percentile(data, 75) - np.percentile(data, 25)\n",
        "\n",
        "print(\"\\nDispersion:\")\n",
        "print(f\"Range: {range_value}\")\n",
        "print(f\"Variance: {variance}\")\n",
        "print(f\"Standard Deviation: {std_dev}\")\n",
        "print(f\"Interquartile Range (IQR): {iqr}\")\n"
      ],
      "metadata": {
        "id": "3Wa_cEsbrGPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  Use NumPy or pandas to summarize a dataset’s descriptive statistics."
      ],
      "metadata": {
        "id": "MmIKzn5vrnpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a dataset\n",
        "np.random.seed(0)\n",
        "data = np.random.normal(loc=5, scale=2, size=100)\n",
        "\n",
        "# Create a pandas Series\n",
        "series = pd.Series(data)\n",
        "\n",
        "# Calculate descriptive statistics\n",
        "stats = series.describe()\n",
        "\n",
        "print(stats)\n"
      ],
      "metadata": {
        "id": "UTch2KNTr_ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Plot a boxplot to understand the spread and identify outliers."
      ],
      "metadata": {
        "id": "-42gxkEBsRrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Create a dataset\n",
        "np.random.seed(0)\n",
        "data = np.random.normal(loc=5, scale=2, size=100)\n",
        "\n",
        "# Add some outliers\n",
        "data = np.append(data, [15, 16, 17])\n",
        "\n",
        "# Create a boxplot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.boxplot(data, vert=False)\n",
        "plt.title(\"Boxplot of Data\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "G_zq4muNsq0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Calculate the interquartile range (IQR) of a dataset."
      ],
      "metadata": {
        "id": "CITBTc1xtPY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create a dataset\n",
        "np.random.seed(0)\n",
        "data = np.random.normal(loc=5, scale=2, size=100)\n",
        "\n",
        "# Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
        "Q1 = np.percentile(data, 25)\n",
        "Q3 = np.percentile(data, 75)\n",
        "\n",
        "# Calculate IQR\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "print(\"Q1 (25th percentile):\", Q1)\n",
        "print(\"Q3 (75th percentile):\", Q3)\n",
        "print(\"Interquartile Range (IQR):\", IQR)\n"
      ],
      "metadata": {
        "id": "4JAx_-VatV3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.  Implement Z-score normalization and explain its significance."
      ],
      "metadata": {
        "id": "1Q4j8SvctrWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create a dataset\n",
        "np.random.seed(0)\n",
        "data = np.random.normal(loc=5, scale=2, size=100)\n",
        "\n",
        "# Calculate mean and standard deviation\n",
        "mean = np.mean(data)\n",
        "std_dev = np.std(data)\n",
        "\n",
        "# Apply Z-score normalization\n",
        "normalized_data = (data - mean) / std_dev\n",
        "\n",
        "print(\"Original Data Mean:\", np.mean(data))\n",
        "print(\"Original Data Standard Deviation:\", np.std(data))\n",
        "print(\"Normalized Data Mean:\", np.mean(normalized_data))\n",
        "print(\"Normalized Data Standard Deviation:\", np.std(normalized_data))\n"
      ],
      "metadata": {
        "id": "ldtMQO1atwHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Compare two datasets using their standard deviations."
      ],
      "metadata": {
        "id": "Je3xwI__uCnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create two datasets\n",
        "np.random.seed(0)\n",
        "dataset1 = np.random.normal(loc=5, scale=1, size=100)\n",
        "dataset2 = np.random.normal(loc=5, scale=2, size=100)\n",
        "\n",
        "# Calculate standard deviations\n",
        "std_dev1 = np.std(dataset1)\n",
        "std_dev2 = np.std(dataset2)\n",
        "\n",
        "print(\"Dataset 1 Standard Deviation:\", std_dev1)\n",
        "print(\"Dataset 2 Standard Deviation:\", std_dev2)\n",
        "\n",
        "# Compare standard deviations\n",
        "if std_dev1 < std_dev2:\n",
        "    print(\"Dataset 1 has less variability than Dataset 2.\")\n",
        "elif std_dev1 > std_dev2:\n",
        "    print(\"Dataset 1 has more variability than Dataset 2.\")\n",
        "else:\n",
        "    print(\"Both datasets have similar variability.\")\n"
      ],
      "metadata": {
        "id": "R654LUnXuGbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to visualize covariance using a heatmap."
      ],
      "metadata": {
        "id": "jRqWAxMRuTvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a dataset\n",
        "np.random.seed(0)\n",
        "data = np.random.multivariate_normal(mean=[0, 0, 0, 0], cov=[[1, 0.5, 0.2, 0.1], [0.5, 1, 0.3, 0.2], [0.2, 0.3, 1, 0.4], [0.1, 0.2, 0.4, 1]], size=100)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data, columns=[\"Feature1\", \"Feature2\", \"Feature3\", \"Feature4\"])\n",
        "\n",
        "# Calculate covariance matrix\n",
        "cov_matrix = df.cov()\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cov_matrix, annot=True, cmap=\"coolwarm\", square=True)\n",
        "plt.title(\"Covariance Matrix Heatmap\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "s-mAPXimuY9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.  Use seaborn to create a correlation matrix for a dataset."
      ],
      "metadata": {
        "id": "mYbh18uCvEfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a dataset\n",
        "np.random.seed(0)\n",
        "data = np.random.multivariate_normal(mean=[0, 0, 0, 0], cov=[[1, 0.5, 0.2, 0.1], [0.5, 1, 0.3, 0.2], [0.2, 0.3, 1, 0.4], [0.1, 0.2, 0.4, 1]], size=100)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data, columns=[\"Feature1\", \"Feature2\", \"Feature3\", \"Feature4\"])\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", square=True)\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "sMCk-VXTvJRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.  Generate a dataset and implement both variance and standard deviation computations."
      ],
      "metadata": {
        "id": "JtBnKr0Qvd0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate a dataset\n",
        "np.random.seed(0)\n",
        "data = np.random.normal(loc=5, scale=2, size=100)\n",
        "\n",
        "# Compute variance\n",
        "variance = np.var(data)\n",
        "\n",
        "# Compute standard deviation\n",
        "std_dev = np.std(data)\n",
        "\n",
        "print(\"Dataset Mean:\", np.mean(data))\n",
        "print(\"Variance:\", variance)\n",
        "print(\"Standard Deviation:\", std_dev)\n"
      ],
      "metadata": {
        "id": "pPef-OcGvk45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Visualize skewness and kurtosis using Python libraries like matplotlib or seaborn."
      ],
      "metadata": {
        "id": "VgzTSS4cv5gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# Generate skewed data\n",
        "np.random.seed(0)\n",
        "skewed_data = np.random.exponential(scale=1, size=1000)\n",
        "\n",
        "# Generate normal data\n",
        "normal_data = np.random.normal(loc=0, scale=1, size=1000)\n",
        "\n",
        "# Calculate skewness and kurtosis\n",
        "skewness_skewed = stats.skew(skewed_data)\n",
        "kurtosis_skewed = stats.kurtosis(skewed_data)\n",
        "skewness_normal = stats.skew(normal_data)\n",
        "kurtosis_normal = stats.kurtosis(normal_data)\n",
        "\n",
        "print(\"Skewed Data Skewness:\", skewness_skewed)\n",
        "print(\"Skewed Data Kurtosis:\", kurtosis_skewed)\n",
        "print(\"Normal Data Skewness:\", skewness_normal)\n",
        "print(\"Normal Data Kurtosis:\", kurtosis_normal)\n",
        "\n",
        "# Create histograms\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(skewed_data, kde=True)\n",
        "plt.title(\"Skewed Data\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(normal_data, kde=True)\n",
        "plt.title(\"Normal Data\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s7h3NaFxv-hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.  Implement the Pearson and Spearman correlation coefficients for a dataset."
      ],
      "metadata": {
        "id": "OjJZtsA8wz2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "# Generate a dataset\n",
        "np.random.seed(0)\n",
        "data = np.random.multivariate_normal(mean=[0, 0], cov=[[1, 0.7], [0.7, 1]], size=100)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data, columns=[\"Feature1\", \"Feature2\"])\n",
        "\n",
        "# Calculate Pearson correlation coefficient\n",
        "pearson_coef, pearson_p = pearsonr(df[\"Feature1\"], df[\"Feature2\"])\n",
        "print(\"Pearson Correlation Coefficient:\", pearson_coef)\n",
        "print(\"Pearson p-value:\", pearson_p)\n",
        "\n",
        "# Calculate Spearman correlation coefficient\n",
        "spearman_coef, spearman_p = spearmanr(df[\"Feature1\"], df[\"Feature2\"])\n",
        "print(\"Spearman Correlation Coefficient:\", spearman_coef)\n",
        "print(\"Spearman p-value:\", spearman_p)\n"
      ],
      "metadata": {
        "id": "VKNZ59e9xRkH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}